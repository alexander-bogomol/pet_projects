{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.alfa = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, input):\n",
    "        mean = input.mean(dim=-1, keepdim=True)\n",
    "        std = input.std(dim=-1, keepdim=True)\n",
    "        return self.alfa * (input - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0167, -0.6778,  0.5319, -0.9807, -1.0072,  1.1170],\n",
       "          [-1.6710, -0.1143, -0.1853,  0.0722,  1.3696,  0.5288],\n",
       "          [-1.3628,  0.9835,  0.3506, -0.9035,  1.0992, -0.1670]],\n",
       " \n",
       "         [[-0.0858, -0.5054,  0.7010,  1.5698, -1.2704, -0.4093],\n",
       "          [-1.0077,  1.0716, -0.0588, -1.1348,  1.2382, -0.1085],\n",
       "          [ 0.2882, -0.9200, -0.7648, -0.6624,  1.7103,  0.3487]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1137, -0.7424,  0.5826, -1.0742, -1.1033,  1.2236],\n",
       "          [-1.8303, -0.1252, -0.2030,  0.0791,  1.5002,  0.5792],\n",
       "          [-1.4927,  1.0772,  0.3840, -0.9896,  1.2039, -0.1829]],\n",
       " \n",
       "         [[-0.0940, -0.5536,  0.7678,  1.7194, -1.3914, -0.4483],\n",
       "          [-1.1039,  1.1738, -0.0644, -1.2431,  1.3563, -0.1188],\n",
       "          [ 0.3157, -1.0077, -0.8378, -0.7256,  1.8734,  0.3820]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand((2, 3, 6))\n",
    "\n",
    "\n",
    "test_ln = LayerNorm(6)\n",
    "torch_ln = nn.LayerNorm(6)\n",
    "\n",
    "test_ln(z), torch_ln(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None):\n",
    "    hid_dim = q.size(-1)\n",
    "    scores = (q @ k.transpose(-2, -1)) / math.sqrt(hid_dim)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return torch.matmul(scores.softmax(dim=-1), v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, heads, dropout=0):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        The code string bellow is equivalent to this:\n",
    "        self.w_q = nn.Linear(model_dim, model_dim)\n",
    "        self.w_k = nn.Linear(model_dim, model_dim)\n",
    "        self.w_v = nn.Linear(model_dim, model_dim)\n",
    "        self.w_out = nn.Linear(model_dim, model_dim)\n",
    "        \"\"\"\n",
    "        self.linears = nn.ModuleList([nn.Linear(model_dim, model_dim) for _ in range(4)])\n",
    "\n",
    "        assert model_dim % heads == 0, ('In this simple realisation of transformer '\n",
    "                                        'you should observe equation: model_dim % heads == 0')\n",
    "        self.hid_dim = model_dim // heads\n",
    "        self.heads = heads\n",
    "        self.model_dim = model_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bs = query.size(0)\n",
    "        q, k, v = [lin(input).reshape(bs, -1, self.heads, self.hid_dim).transpose(1,2) \\\n",
    "                   for lin, input in zip(self.linears[:3], (query, key, value))]\n",
    "        att = attention(q, k, v, mask)\n",
    "        att = att.transpose(1, 2).reshape(bs, -1, self.hid_dim * self.heads)\n",
    "\n",
    "        return self.dropout(self.linears[-1](att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mha = MultiHeadAttention(512, 8)\n",
    "torch_mha = nn.MultiheadAttention(512, 8, batch_first=True)\n",
    "x = torch.rand((2, 5, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight       (1536, 512)\n",
      "in_proj_bias         (1536,)\n",
      "out_proj.weight      (512, 512)\n",
      "out_proj.bias        (512,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in torch_mha.state_dict().items():\n",
    "    print(f'{k:20} {tuple(v.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = torch_mha.in_proj_weight[:512], torch_mha.in_proj_weight[512:512+512], torch_mha.in_proj_weight[512+512:]\n",
    "q_b, k_b, v_b = torch_mha.in_proj_bias[:512], torch_mha.in_proj_bias[512:512+512], torch_mha.in_proj_bias[512+512:]\n",
    "out, out_b = torch_mha.out_proj.weight, torch_mha.out_proj.bias\n",
    "\n",
    "for param, weight in zip(test_mha.linears, (q, k, v, out)):\n",
    "    param.weight.data = weight\n",
    "\n",
    "for param, bias in zip(test_mha.linears, (q_b, k_b, v_b, out_b)):\n",
    "    param.bias.data = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch_mha(x, x, x, need_weights=False)[0].data\n",
    "b = test_mha(x, x, x).data\n",
    "torch.allclose(a, b, rtol=1e-04, atol=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hid_dim=None, dropout=0):\n",
    "        super().__init__()\n",
    "        hid_dim = model_dim if hid_dim == None else hid_dim\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, model_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0, 'd_model must be even'\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, model_dim, padding_idx)\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * math.sqrt(self.model_dim)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_dim, heads, ff_hid_dim=None, dropout=0):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(model_dim, heads, dropout)\n",
    "        self.LN_1 = LayerNorm(model_dim)\n",
    "        self.FF = FeedForward(model_dim, ff_hid_dim, dropout)\n",
    "        self.LN_2 = LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        out_1 = x + self.MHA(x, x, x)\n",
    "        out_1 = self.LN_1(out_1)\n",
    "        \n",
    "        out_2 = out_1 + self.FF(out_1)\n",
    "        out_2 = self.LN_2(out_2)\n",
    "        return out_2\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, model_dim, heads, ff_hid_dim=None, dropout=0):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(model_dim, heads, dropout)\n",
    "        self.LN_1 = LayerNorm(model_dim)\n",
    "        self.EDA = MultiHeadAttention(model_dim, heads, dropout)\n",
    "        self.LN_2 = LayerNorm(model_dim)\n",
    "        self.FF = FeedForward(model_dim, ff_hid_dim, dropout)\n",
    "        self.LN_3 = LayerNorm(model_dim)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, enc_out, mask=None):\n",
    "        out_1 = x + self.MHA(x, x, x)\n",
    "        out_1 = self.LN_1(input=out_1)\n",
    "\n",
    "        out_2 = out_1 + self.EDA(out_1, enc_out, enc_out)\n",
    "        out_2 = self.LN_2(input=out_2)\n",
    "\n",
    "        out_3 = out_2 + self.FF(out_2)\n",
    "        out_3 = self.LN_3(input=out_3)\n",
    "        return out_3\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_enc, model_dim, heads, src_vocab_size, tg_vocab_size, \n",
    "                 max_len=5000, ff_hid_dim=None, dropout=0, num_dec=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_emb = nn.Sequential(\n",
    "            Embeddings(src_vocab_size, model_dim, padding_idx=0),\n",
    "            PositionalEncoding(model_dim, dropout, max_len)\n",
    "        )\n",
    "        self.dec_emb = nn.Sequential(\n",
    "            Embeddings(tg_vocab_size, model_dim, padding_idx=0),\n",
    "            PositionalEncoding(model_dim, dropout, max_len)\n",
    "        )\n",
    "        num_dec = num_enc if num_dec == None else num_dec\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [Encoder(model_dim, heads, ff_hid_dim, dropout) for _ in range(num_enc)]\n",
    "            )\n",
    "        self.decoders = nn.ModuleList(\n",
    "            [Decoder(model_dim, heads, ff_hid_dim, dropout) for _ in range(num_dec)]\n",
    "            )\n",
    "\n",
    "    def forward(self, enc_input, dec_input, src_mask=None, tg_mask=None):\n",
    "        memory = self.encode(enc_input, src_mask=None)\n",
    "        out = self.decode(dec_input, memory, tg_mask)\n",
    "        return out\n",
    "\n",
    "    def encode(self, input, src_mask=None):\n",
    "        x_enc = self.enc_emb(input)\n",
    "        for enc in self.encoders:\n",
    "            x_enc = enc(x_enc, src_mask)\n",
    "        return x_enc\n",
    "\n",
    "    def decode(self, input, memory, tg_mask=None):\n",
    "        x_dec = self.dec_emb(input)\n",
    "        for dec in self.decoders:\n",
    "            x_dec = dec(x_dec, memory, tg_mask)\n",
    "        return x_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "subsequent_mask(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 512])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(512, 8, dropout=0.1)\n",
    "x = torch.rand((3, 4, 512))\n",
    "enc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abv = EncoderDecoder(num_enc=3, model_dim=512, heads=8, dropout=0.2, src_vocab_size=20, tg_vocab_size=22)\n",
    "x = torch.randint(0, 20, (1, 6))\n",
    "y = torch.randint(0, 22, (1, 2))\n",
    "abv(x, y, tg_mask=subsequent_mask(2)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74c7431f4435830a057422cf59695882c87dc0edd1b77534c7869fc1258ccd33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
