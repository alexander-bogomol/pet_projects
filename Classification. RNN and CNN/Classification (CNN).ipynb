{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2829cd4d-78ba-4150-917b-a4199fcf6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db04756a-d727-4447-b2a7-442292f7d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, target_transform=None):\n",
    "        self.csv_data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.csv_data.iloc[idx, 1]\n",
    "        label = self.csv_data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            text = torch.tensor(self.transform(text), dtype=torch.int64)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c6b927-a083-488b-9d49-c2fa260cf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(csv_file=r'IMDB_train.csv')\n",
    "valid_dataset = CustomDataset(csv_file=r'IMDB_valid.csv')\n",
    "test_dataset = CustomDataset(csv_file=r'IMDB_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d221a6ff-9921-44d3-b7f4-b0f8771f07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_df = pd.read_csv(r'IMDB_train.csv')\n",
    "train_tokenized = train_df['texts'].map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94d236a-f174-4ebb-a979-42efbc296412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(train_tokenized, specials=['<pad>', '<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff5311c-1699-403a-8bfa-ff1ac8029409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1826b680-5849-4d62-9bbd-231f1273529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch_text2id_seq = [torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64) for _text, _ in batch]\n",
    "    batch_label_seq = torch.tensor([_label for _, _label in batch], dtype=torch.float)\n",
    "    batch_label_seq = batch_label_seq.unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    batch_padded = pad_sequence(batch_text2id_seq, batch_first=True)\n",
    "    return batch_padded.to(device), batch_label_seq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5637115-7307-49ba-8a15-8e6fe51dad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        opt.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = loss_func(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        opt.step()\n",
    "        total_acc += ((torch.sigmoid(predicted_label) > 0.5) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f} | lr: {:.0e}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count, scheduler.get_last_lr()[0]))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            total_acc += ((torch.sigmoid(predicted_label) > 0.5) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "        return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02b0170d-29dd-4e2c-ad34-40e044de7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim,\n",
    "        out_channels,\n",
    "        kernel_sizes,\n",
    "        padding_idx,\n",
    "        padding=[0, 0, 0],\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx)\n",
    "\n",
    "        self.conv_0 = nn.Conv1d(emb_dim, out_channels[0], kernel_sizes[0], padding=padding[0])\n",
    "        self.conv_1 = nn.Conv1d(emb_dim, out_channels[1], kernel_sizes[1], padding=padding[1])\n",
    "        self.conv_2 = nn.Conv1d(emb_dim, out_channels[2], kernel_sizes[2], padding=padding[2])\n",
    "        \n",
    "        self.fc = nn.Linear(torch.tensor(out_channels).sum(), 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded))\n",
    "        conved_1 = F.relu(self.conv_1(embedded))\n",
    "        conved_2 = F.relu(self.conv_2(embedded))\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64f1b90f-1cbf-4e92-8e48-a8de218100fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 200\n",
    "out_channels = [256, 256, 256]\n",
    "kernel_sizes = [2, 3, 5]\n",
    "padding=[1, 2, 3]\n",
    "dropout=0.5\n",
    "padding_idx=vocab['<pad>']\n",
    "\n",
    "\n",
    "model = CNN(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    out_channels=out_channels,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout=dropout,\n",
    "    padding=padding,\n",
    "    padding_idx=padding_idx\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, 1.0, gamma=0.1)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "max_epochs = 20\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b2809d2-b81f-4b3c-bd98-6e8cdc0d40cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  274 batches | accuracy    0.647 | lr: 1e-03\n",
      "| epoch   1 |   200/  274 batches | accuracy    0.764 | lr: 1e-03\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.17s | valid accuracy    0.849 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   100/  274 batches | accuracy    0.797 | lr: 1e-03\n",
      "| epoch   2 |   200/  274 batches | accuracy    0.829 | lr: 1e-03\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.15s | valid accuracy    0.879 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   100/  274 batches | accuracy    0.831 | lr: 1e-03\n",
      "| epoch   3 |   200/  274 batches | accuracy    0.853 | lr: 1e-03\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.24s | valid accuracy    0.884 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   100/  274 batches | accuracy    0.848 | lr: 1e-03\n",
      "| epoch   4 |   200/  274 batches | accuracy    0.882 | lr: 1e-03\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.10s | valid accuracy    0.860 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   100/  274 batches | accuracy    0.862 | lr: 1e-04\n",
      "| epoch   5 |   200/  274 batches | accuracy    0.914 | lr: 1e-04\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.29s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   100/  274 batches | accuracy    0.864 | lr: 1e-05\n",
      "| epoch   6 |   200/  274 batches | accuracy    0.921 | lr: 1e-05\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.22s | valid accuracy    0.899 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   100/  274 batches | accuracy    0.874 | lr: 1e-05\n",
      "| epoch   7 |   200/  274 batches | accuracy    0.921 | lr: 1e-05\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.03s | valid accuracy    0.900 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   100/  274 batches | accuracy    0.882 | lr: 1e-05\n",
      "| epoch   8 |   200/  274 batches | accuracy    0.922 | lr: 1e-05\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.00s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   100/  274 batches | accuracy    0.876 | lr: 1e-05\n",
      "| epoch   9 |   200/  274 batches | accuracy    0.925 | lr: 1e-05\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.16s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   100/  274 batches | accuracy    0.882 | lr: 1e-05\n",
      "| epoch  10 |   200/  274 batches | accuracy    0.922 | lr: 1e-05\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.36s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   100/  274 batches | accuracy    0.876 | lr: 1e-06\n",
      "| epoch  11 |   200/  274 batches | accuracy    0.922 | lr: 1e-06\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.21s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   100/  274 batches | accuracy    0.879 | lr: 1e-06\n",
      "| epoch  12 |   200/  274 batches | accuracy    0.923 | lr: 1e-06\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time: 12.24s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   100/  274 batches | accuracy    0.885 | lr: 1e-06\n",
      "| epoch  13 |   200/  274 batches | accuracy    0.923 | lr: 1e-06\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.17s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   100/  274 batches | accuracy    0.882 | lr: 1e-06\n",
      "| epoch  14 |   200/  274 batches | accuracy    0.921 | lr: 1e-06\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.32s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   100/  274 batches | accuracy    0.887 | lr: 1e-06\n",
      "| epoch  15 |   200/  274 batches | accuracy    0.924 | lr: 1e-06\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time: 12.20s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   100/  274 batches | accuracy    0.885 | lr: 1e-07\n",
      "| epoch  16 |   200/  274 batches | accuracy    0.924 | lr: 1e-07\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.17s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   100/  274 batches | accuracy    0.881 | lr: 1e-08\n",
      "| epoch  17 |   200/  274 batches | accuracy    0.923 | lr: 1e-08\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.16s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   100/  274 batches | accuracy    0.882 | lr: 1e-09\n",
      "| epoch  18 |   200/  274 batches | accuracy    0.925 | lr: 1e-09\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.19s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   100/  274 batches | accuracy    0.884 | lr: 1e-10\n",
      "| epoch  19 |   200/  274 batches | accuracy    0.924 | lr: 1e-10\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.25s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   100/  274 batches | accuracy    0.880 | lr: 1e-11\n",
      "| epoch  20 |   200/  274 batches | accuracy    0.926 | lr: 1e-11\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.35s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "   \n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08f6f54d-f1aa-4276-ab37-ee6607ad5db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54db5e5a-9019-4943-a0d3-2ae0983ff276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983785152435303"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ev(sen):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(sen)\n",
    "        inp = torch.tensor(vocab(tokenized)).unsqueeze(0).to(device)\n",
    "        pred = torch.sigmoid(model(inp))\n",
    "    return pred.item()\n",
    "ev('It was a fantastic performance !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b839b43-9aa8-4743-a636-fca95313e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9947662353515625\n",
      "0.9979516863822937\n",
      "0.010136223398149014\n",
      "0.06495773792266846\n",
      "0.0990145280957222\n",
      "0.5555111765861511\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Best film ever', \n",
    "             'Such a great show!', \n",
    "             'It was a horrible movie', \n",
    "             'I\\'ve never watched something as bad', \n",
    "             'It is a disgusting movie!', \n",
    "             'So-so. I\\'d watched something better you know']\n",
    "for sen in sentences:\n",
    "    print(ev(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b38ce-ef3b-45fe-80d4-0e1beb1823db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
